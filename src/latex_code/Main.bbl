% Generated by IEEEtran.bst, version: 1.12 (2007/01/11)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{liu2022overview}
R.~Liu, A.~A. Ramli, H.~Zhang, E.~Henricson, and X.~Liu, ``An overview of human
  activity recognition using wearable sensors: Healthcare and artificial
  intelligence,'' in \emph{Internet of Things--ICIOT 2021: 6th International
  Conference, Held as Part of the Services Conference Federation, SCF 2021,
  Virtual Event, December 10--14, 2021, Proceedings}.\hskip 1em plus 0.5em
  minus 0.4em\relax Springer, 2022, pp. 1--14.

\bibitem{pathan2019machine}
N.~S. Pathan, M.~T.~F. Talukdar, M.~Quamruzzaman, and S.~A. Fattah, ``A machine
  learning based human activity recognition during physical exercise using
  wavelet packet transform of ppg and inertial sensors data,'' in \emph{2019
  4th International Conference on Electrical Information and Communication
  Technology (EICT)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2019, pp.
  1--5.

\bibitem{saini2020human}
R.~Saini and V.~Maan, ``Human activity and gesture recognition: A review,'' in
  \emph{2020 International Conference on Emerging Trends in Communication,
  Control and Computing (ICONC3)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
  2020, pp. 1--2.

\bibitem{fan2022context}
L.~Fan, P.~Delir~Haghighi, Y.~Zhang, A.~R.~M. Forkan, and P.~P. Jayaraman,
  ``Context-aware human activity recognition (ca-har) using smartphone built-in
  sensors,'' in \emph{Advances in Mobile Computing and Multimedia Intelligence:
  20th International Conference, MoMM 2022, Virtual Event, November 28--30,
  2022, Proceedings}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2022,
  pp. 108--121.

\bibitem{kumari2017increasing}
P.~Kumari, L.~Mathew, and P.~Syal, ``Increasing trend of wearables and
  multimodal interface for human activity monitoring: A review,''
  \emph{Biosensors and Bioelectronics}, vol.~90, pp. 298--307, 2017.

\bibitem{cippitelli2017human}
E.~Cippitelli, E.~Gambi, and S.~Spinsante, ``Human action recognition with
  rgb-d sensors,'' \emph{Motion Tracking and Gesture Recognition}, vol.~97,
  2017.

\bibitem{taylor2010convolutional}
G.~W. Taylor, R.~Fergus, Y.~LeCun, and C.~Bregler, ``Convolutional learning of
  spatio-temporal features,'' in \emph{European conference on computer
  vision}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2010, pp. 140--153.

\bibitem{qiu2017learning}
Z.~Qiu, T.~Yao, and T.~Mei, ``Learning spatio-temporal representation with
  pseudo-3d residual networks,'' in \emph{proceedings of the IEEE International
  Conference on Computer Vision}, 2017, pp. 5533--5541.

\bibitem{carreira2017quo}
J.~Carreira and A.~Zisserman, ``Quo vadis, action recognition? a new model and
  the kinetics dataset,'' in \emph{proceedings of the IEEE Conference on
  Computer Vision and Pattern Recognition}, 2017, pp. 6299--6308.

\bibitem{feichtenhofer2019slowfast}
C.~Feichtenhofer, H.~Fan, J.~Malik, and K.~He, ``Slowfast networks for video
  recognition,'' in \emph{Proceedings of the IEEE/CVF international conference
  on computer vision}, 2019, pp. 6202--6211.

\bibitem{diete2019vision}
A.~Diete, T.~Sztyler, and H.~Stuckenschmidt, ``Vision and acceleration
  modalities: Partners for recognizing complex activities,'' in \emph{2019 IEEE
  International Conference on Pervasive Computing and Communications Workshops
  (PerCom Workshops)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2019, pp.
  101--106.

\bibitem{chen2014home}
C.~Chen, K.~Liu, R.~Jafari, and N.~Kehtarnavaz, ``Home-based senior fitness
  test measurement system using collaborative inertial and depth sensors,'' in
  \emph{2014 36th Annual International Conference of the IEEE Engineering in
  Medicine and Biology Society}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
  2014, pp. 4135--4138.

\bibitem{khandnor2017survey}
P.~Khandnor, N.~Kumar \emph{et~al.}, ``A survey of activity recognition process
  using inertial sensors and smartphone sensors,'' in \emph{2017 International
  Conference on Computing, Communication and Automation (ICCCA)}.\hskip 1em
  plus 0.5em minus 0.4em\relax IEEE, 2017, pp. 607--612.

\bibitem{zhao2018deep}
Y.~Zhao, R.~Yang, G.~Chevalier, X.~Xu, and Z.~Zhang, ``Deep residual bidir-lstm
  for human activity recognition using wearable sensors,'' \emph{Mathematical
  Problems in Engineering}, vol. 2018, pp. 1--13, 2018.

\bibitem{al2018hierarchical}
M.~Al-Naser, H.~Ohashi, S.~Ahmed, K.~Nakamura, T.~Akiyama, T.~Sato, P.~X.
  Nguyen, and A.~Dengel, ``Hierarchical model for zero-shot activity
  recognition using wearable sensors.'' in \emph{ICAART (2)}, 2018, pp.
  478--485.

\bibitem{gao2021danhar}
W.~Gao, L.~Zhang, Q.~Teng, J.~He, and H.~Wu, ``Danhar: Dual attention network
  for multimodal human activity recognition using wearable sensors,''
  \emph{Applied Soft Computing}, vol. 111, p. 107728, 2021.

\bibitem{dang2020sensor}
L.~M. Dang, K.~Min, H.~Wang, M.~J. Piran, C.~H. Lee, and H.~Moon,
  ``Sensor-based and vision-based human activity recognition: A comprehensive
  survey,'' \emph{Pattern Recognition}, vol. 108, p. 107561, 2020.

\bibitem{ravi2016deep}
D.~Ravi, C.~Wong, B.~Lo, and G.-Z. Yang, ``Deep learning for human activity
  recognition: A resource efficient implementation on low-power devices,'' in
  \emph{2016 IEEE 13th international conference on wearable and implantable
  body sensor networks (BSN)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
  2016, pp. 71--76.

\bibitem{atrey2010multimodal}
P.~K. Atrey, M.~A. Hossain, A.~El~Saddik, and M.~S. Kankanhalli, ``Multimodal
  fusion for multimedia analysis: a survey,'' \emph{Multimedia systems},
  vol.~16, pp. 345--379, 2010.

\bibitem{moencks2019adaptive}
M.~Moencks, V.~De~Silva, J.~Roche, and A.~Kondoz, ``Adaptive feature processing
  for robust human activity recognition on a novel multi-modal dataset,''
  \emph{arXiv preprint arXiv:1901.02858}, 2019.

\bibitem{shahroudy2017deep}
A.~Shahroudy, T.-T. Ng, Y.~Gong, and G.~Wang, ``Deep multimodal feature
  analysis for action recognition in rgb+ d videos,'' \emph{IEEE transactions
  on pattern analysis and machine intelligence}, vol.~40, no.~5, pp.
  1045--1058, 2017.

\bibitem{islam2020hamlet}
M.~M. Islam and T.~Iqbal, ``Hamlet: A hierarchical multimodal attention-based
  human activity recognition algorithm,'' in \emph{2020 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS)}.\hskip 1em plus 0.5em
  minus 0.4em\relax IEEE, 2020, pp. 10\,285--10\,292.

\bibitem{islam2022mumu}
M.~M. Islam and T.~Iqba, ``Mumu: Cooperative multitask learning-based guided
  multimodal fusion,'' in \emph{Proceedings of the AAAI Conference on
  Artificial Intelligence}, vol.~36, no.~1, 2022, pp. 1043--1051.

\bibitem{ni2022cross}
J.~Ni, R.~Sarbajna, Y.~Liu, A.~H. Ngu, and Y.~Yan, ``Cross-modal knowledge
  distillation for vision-to-sensor action recognition,'' in \emph{ICASSP
  2022-2022 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2022, pp.
  4448--4452.

\bibitem{islam2022maven}
M.~M. Islam, M.~S. Yasar, and T.~Iqbal, ``Maven: A memory augmented recurrent
  approach for multimodal fusion,'' \emph{IEEE Transactions on Multimedia}, pp.
  1--1, 2022.

\bibitem{zhou2020unified}
L.~Zhou, H.~Palangi, L.~Zhang, H.~Hu, J.~Corso, and J.~Gao, ``{Unified
  vision-language pre-training for image captioning and VQA},'' in
  \emph{Proceedings of the AAAI conference on artificial intelligence},
  vol.~34, no.~07, 2020, pp. 13\,041--13\,049.

\bibitem{chen2015utd}
C.~Chen, R.~Jafari, and N.~Kehtarnavaz, ``Utd-mhad: A multimodal dataset for
  human action recognition utilizing a depth camera and a wearable inertial
  sensor,'' in \emph{2015 IEEE International conference on image processing
  (ICIP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2015, pp. 168--172.

\bibitem{shahroudy2016ntu}
A.~Shahroudy, J.~Liu, T.-T. Ng, and G.~Wang, ``Ntu rgb+ d: A large scale
  dataset for 3d human activity analysis,'' in \emph{Proceedings of the IEEE
  conference on computer vision and pattern recognition}, 2016, pp. 1010--1019.

\bibitem{dawar2018action}
N.~{Dawar} and N.~Kehtarnavaz, ``Action detection and recognition in continuous
  action streams by deep learning-based sensing fusion,'' \emph{IEEE Sensors
  Journal}, vol.~18, no.~23, pp. 9660--9668, 2018.

\bibitem{dawar2018data}
N.~Dawar, S.~Ostadabbas, and N.~Kehtarnavaz, ``Data augmentation in deep
  learning-based fusion of depth and inertial sensing for action recognition,''
  \emph{IEEE Sensors Letters}, vol.~3, no.~1, pp. 1--4, 2018.

\bibitem{imran2020evaluating}
J.~Imran and B.~Raman, ``Evaluating fusion of rgb-d and inertial sensors for
  multimodal human action recognition,'' \emph{Journal of Ambient Intelligence
  and Humanized Computing}, vol.~11, no.~1, pp. 189--208, 2020.

\bibitem{zou2019wifi}
H.~Zou, J.~Yang, H.~P. Das, H.~Liu, Y.~Zhou, and C.~J. Spanos, ``Wifi and
  vision multimodal learning for accurate and robust device-free human activity
  recognition,'' in \emph{2019 IEEE/CVF Conference on Computer Vision and
  Pattern Recognition Workshops (CVPRW)}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2019, pp. 426--433.

\bibitem{tran2015learning}
D.~Tran, L.~Bourdev, R.~Fergus, L.~Torresani, and M.~Paluri, ``Learning
  spatiotemporal features with 3d convolutional networks,'' in
  \emph{Proceedings of the IEEE international conference on computer vision},
  2015, pp. 4489--4497.

\bibitem{long2018multimodal}
X.~Long, C.~Gan, G.~Melo, X.~Liu, Y.~Li, F.~Li, and S.~Wen, ``Multimodal
  keyless attention fusion for video classification,'' in \emph{Proceedings of
  the AAAI Conference on Artificial Intelligence}, vol.~32, no.~1, 2018.

\bibitem{zhou2018temporal}
B.~Zhou, A.~Andonian, A.~Oliva, and A.~Torralba, ``Temporal relational
  reasoning in videos,'' in \emph{Proceedings of the European conference on
  computer vision (ECCV)}, 2018, pp. 803--818.

\bibitem{bromley1993signature}
J.~Bromley, I.~Guyon, Y.~LeCun, E.~S{\"a}ckinger, and R.~Shah, ``Signature
  verification using a" siamese" time delay neural network,'' \emph{Advances in
  neural information processing systems}, vol.~6, 1993.

\bibitem{chopra2005learning}
S.~Chopra, R.~Hadsell, and Y.~LeCun, ``Learning a similarity metric
  discriminatively, with application to face verification,'' in \emph{2005 IEEE
  Computer Society Conference on Computer Vision and Pattern Recognition
  (CVPR'05)}, vol.~1.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2005, pp.
  539--546.

\bibitem{hadsell2006dimensionality}
R.~Hadsell, S.~Chopra, and Y.~LeCun, ``Dimensionality reduction by learning an
  invariant mapping,'' in \emph{2006 IEEE Computer Society Conference on
  Computer Vision and Pattern Recognition (CVPR'06)}, vol.~2.\hskip 1em plus
  0.5em minus 0.4em\relax IEEE, 2006, pp. 1735--1742.

\bibitem{weinberger2009distance}
K.~Q. Weinberger and L.~K. Saul, ``Distance metric learning for large margin
  nearest neighbor classification.'' \emph{Journal of machine learning
  research}, vol.~10, no.~2, 2009.

\bibitem{collobert2008unified}
R.~Collobert and J.~Weston, ``A unified architecture for natural language
  processing: Deep neural networks with multitask learning,'' in
  \emph{Proceedings of the 25th international conference on Machine learning},
  2008, pp. 160--167.

\bibitem{chechik2010large}
G.~Chechik, V.~Sharma, U.~Shalit, and S.~Bengio, ``Large scale online learning
  of image similarity through ranking.'' \emph{Journal of Machine Learning
  Research}, vol.~11, no.~3, 2010.

\bibitem{he2020momentum}
K.~He, H.~Fan, Y.~Wu, S.~Xie, and R.~Girshick, ``Momentum contrast for
  unsupervised visual representation learning,'' in \emph{Proceedings of the
  IEEE/CVF conference on computer vision and pattern recognition}, 2020, pp.
  9729--9738.

\bibitem{chen2020improved}
X.~Chen, H.~Fan, R.~Girshick, and K.~He, ``Improved baselines with momentum
  contrastive learning,'' \emph{arXiv preprint arXiv:2003.04297}, 2020.

\bibitem{chen2020simple}
T.~Chen, S.~Kornblith, M.~Norouzi, and G.~Hinton, ``A simple framework for
  contrastive learning of visual representations,'' in \emph{International
  conference on machine learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR,
  2020, pp. 1597--1607.

\bibitem{chen2020big}
T.~Chen, S.~Kornblith, K.~Swersky, M.~Norouzi, and G.~E. Hinton, ``Big
  self-supervised models are strong semi-supervised learners,'' \emph{Advances
  in neural information processing systems}, vol.~33, pp. 22\,243--22\,255,
  2020.

\bibitem{grill2020bootstrap}
J.-B. Grill, F.~Strub, F.~Altch{\'e}, C.~Tallec, P.~Richemond, E.~Buchatskaya,
  C.~Doersch, B.~Avila~Pires, Z.~Guo, M.~Gheshlaghi~Azar \emph{et~al.},
  ``Bootstrap your own latent-a new approach to self-supervised learning,''
  \emph{Advances in neural information processing systems}, vol.~33, pp.
  21\,271--21\,284, 2020.

\bibitem{kenton2019bert}
J.~D. M.-W.~C. Kenton and L.~K. Toutanova, ``Bert: Pre-training of deep
  bidirectional transformers for language understanding,'' in \emph{Proceedings
  of NAACL-HLT}, 2019, pp. 4171--4186.

\bibitem{brown2020language}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell \emph{et~al.}, ``Language
  models are few-shot learners,'' \emph{Advances in neural information
  processing systems}, vol.~33, pp. 1877--1901, 2020.

\bibitem{dosovitskiy2020image}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai,
  T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly \emph{et~al.},
  ``An image is worth 16x16 words: Transformers for image recognition at
  scale,'' \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{hu2021unit}
R.~Hu and A.~Singh, ``Unit: Multimodal multitask learning with a unified
  transformer,'' in \emph{Proceedings of the IEEE/CVF International Conference
  on Computer Vision}, 2021, pp. 1439--1449.

\bibitem{wang2021ufo}
J.~Wang, X.~Hu, Z.~Gan, Z.~Yang, X.~Dai, Z.~Liu, Y.~Lu, and L.~Wang, ``Ufo: A
  unified transformer for vision-language representation learning,''
  \emph{arXiv preprint arXiv:2111.10023}, 2021.

\bibitem{li2021uniformer}
K.~Li, Y.~Wang, G.~Peng, G.~Song, Y.~Liu, H.~Li, and Y.~Qiao, ``Uniformer:
  Unified transformer for efficient spatial-temporal representation learning,''
  in \emph{International Conference on Learning Representations}, 2021.

\bibitem{ma2022unitranser}
Z.~Ma, J.~Li, G.~Li, and Y.~Cheng, ``Unitranser: A unified transformer semantic
  representation framework for multimodal task-oriented dialog system,'' in
  \emph{Proceedings of the 60th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, 2022, pp. 103--114.

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin, ``Attention is all you need,''
  \emph{Advances in neural information processing systems}, vol.~30, 2017.

\bibitem{lin2019tsm}
J.~Lin, C.~Gan, and S.~Han, ``Tsm: Temporal shift module for efficient video
  understanding,'' in \emph{Proceedings of the IEEE/CVF international
  conference on computer vision}, 2019, pp. 7083--7093.

\bibitem{imran2016human}
J.~Imran and P.~Kumar, ``Human action recognition using rgb-d sensor and deep
  convolutional neural networks,'' in \emph{2016 international conference on
  advances in computing, communications and informatics (ICACCI)}.\hskip 1em
  plus 0.5em minus 0.4em\relax IEEE, 2016, pp. 144--148.

\bibitem{liu2018rgb}
T.~Liu, J.~Kong, and M.~Jiang, ``Rgb-d action recognition using multimodal
  correlative representation learning model,'' \emph{IEEE Sensors Journal},
  vol.~19, no.~5, pp. 1862--1872, 2018.

\bibitem{yu2018spatio}
B.~Yu, H.~Yin, and Z.~Zhu, ``Spatio-temporal graph convolutional networks: a
  deep learning framework for traffic forecasting,'' in \emph{Proceedings of
  the 27th International Joint Conference on Artificial Intelligence}, 2018,
  pp. 3634--3640.

\bibitem{singh2020deep}
S.~P. Singh, M.~K. Sharma, A.~Lay-Ekuakille, D.~Gangwar, and S.~Gupta, ``Deep
  convlstm with self-attention for human activity decoding using wearable
  sensors,'' \emph{IEEE Sensors Journal}, vol.~21, no.~6, pp. 8575--8582, 2020.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Deep residual learning for image
  recognition,'' in \emph{Proceedings of the IEEE conference on computer vision
  and pattern recognition}, 2016, pp. 770--778.

\bibitem{liu2018recognizing}
M.~Liu and J.~Yuan, ``Recognizing human actions as the evolution of pose
  estimation maps,'' in \emph{Proceedings of the IEEE Conference on Computer
  Vision and Pattern Recognition}, 2018, pp. 1159--1168.

\bibitem{zhao2019bayesian}
R.~Zhao, W.~Xu, H.~Su, and Q.~Ji, ``Bayesian hierarchical dynamic model for
  human action recognition,'' in \emph{Proceedings of the IEEE/CVF Conference
  on Computer Vision and Pattern Recognition}, 2019, pp. 7733--7742.

\bibitem{peng2019correlation}
B.~Peng, X.~Jin, J.~Liu, D.~Li, Y.~Wu, Y.~Liu, S.~Zhou, and Z.~Zhang,
  ``Correlation congruence for knowledge distillation,'' in \emph{Proceedings
  of the IEEE/CVF International Conference on Computer Vision}, 2019, pp.
  5007--5016.

\bibitem{memmesheimer2020gimme}
R.~Memmesheimer, N.~Theisen, and D.~Paulus, ``Gimme signals: Discriminative
  signal encoding for multimodal activity recognition,'' in \emph{2020 IEEE/RSJ
  International Conference on Intelligent Robots and Systems (IROS)}.\hskip 1em
  plus 0.5em minus 0.4em\relax IEEE, 2020, pp. 10\,394--10\,401.

\bibitem{chen2020convnets}
Y.~Chen, L.~Wang, C.~Li, Y.~Hou, and W.~Li, ``Convnets-based action recognition
  from skeleton motion maps,'' \emph{Multimedia Tools and Applications},
  vol.~79, pp. 1707--1725, 2020.

\bibitem{zhu2020exploring}
A.~Zhu, Q.~Wu, R.~Cui, T.~Wang, W.~Hang, G.~Hua, and H.~Snoussi, ``Exploring a
  rich spatial--temporal dependent relational model for skeleton-based action
  recognition by bidirectional lstm-cnn,'' \emph{Neurocomputing}, vol. 414, pp.
  90--100, 2020.

\bibitem{duhme2021fusion}
M.~Duhme, R.~Memmesheimer, and D.~Paulus, ``Fusion-gcn: Multimodal action
  recognition using graph convolutional networks,'' in \emph{DAGM German
  Conference on Pattern Recognition}.\hskip 1em plus 0.5em minus 0.4em\relax
  Springer, 2021, pp. 265--281.

\bibitem{khezerlou2023multi}
F.~Khezerlou, A.~Baradarani, M.~A. Balafar, and R.~G. Maev, ``Multi-stream cnns
  with orientation-magnitude response maps and weighted inception module for
  human action recognition,'' in \emph{2023 3rd International conference on
  Artificial Intelligence and Signal Processing (AISP)}.\hskip 1em plus 0.5em
  minus 0.4em\relax IEEE, 2023, pp. 1--5.

\bibitem{bruce2021multimodal}
X.~Bruce, Y.~Liu, and K.~C. Chan, ``Multimodal fusion via teacher-student
  network for indoor action recognition,'' in \emph{Proceedings of the AAAI
  Conference on Artificial Intelligence}, vol.~35, no.~4, 2021, pp. 3199--3207.

\bibitem{davoodikakhki2020hierarchical}
M.~Davoodikakhki and K.~Yin, ``Hierarchical action classification with network
  pruning,'' in \emph{Advances in Visual Computing: 15th International
  Symposium, ISVC 2020, San Diego, CA, USA, October 5--7, 2020, Proceedings,
  Part I 15}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2020, pp.
  291--305.

\bibitem{das2020vpn}
S.~Das, S.~Sharma, R.~Dai, F.~Bremond, and M.~Thonnat, ``Vpn: Learning
  video-pose embedding for activities of daily living,'' in \emph{Computer
  Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28,
  2020, Proceedings, Part IX 16}.\hskip 1em plus 0.5em minus 0.4em\relax
  Springer, 2020, pp. 72--90.

\bibitem{de2020infrared}
A.~M. De~Boissiere and R.~Noumeir, ``Infrared and 3d skeleton feature fusion
  for rgb-d action recognition,'' \emph{IEEE Access}, vol.~8, pp.
  168\,297--168\,308, 2020.

\bibitem{chi2022infogcn}
H.-g. Chi, M.~H. Ha, S.~Chi, S.~W. Lee, Q.~Huang, and K.~Ramani, ``Infogcn:
  Representation learning for human skeleton-based action recognition,'' in
  \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, 2022, pp. 20\,186--20\,196.

\bibitem{duan2022revisiting}
H.~Duan, Y.~Zhao, K.~Chen, D.~Lin, and B.~Dai, ``Revisiting skeleton-based
  action recognition,'' in \emph{Proceedings of the IEEE/CVF Conference on
  Computer Vision and Pattern Recognition}, 2022, pp. 2969--2978.

\bibitem{ahn2023star}
D.~Ahn, S.~Kim, H.~Hong, and B.~C. Ko, ``Star-transformer: A spatio-temporal
  cross attention transformer for human action recognition,'' in
  \emph{Proceedings of the IEEE/CVF Winter Conference on Applications of
  Computer Vision}, 2023, pp. 3330--3339.

\end{thebibliography}
