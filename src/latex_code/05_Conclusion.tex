In this paper, we presented the UCFFormer, an innovative approach to feature-level fusion designed for HAR task. Employing two core components, FTMT  and MCANet, the UCFFormer established a solid framework for achieving effective multimodal fusion.
UCFFormer incrementally refines the embedding features extracted from each modality, leveraging both FTMT and MCANet.
FTMT captures the high-level inter-dependencies of embedding features spanning both time and modality domains. Utilizing the Factorized Time-Modality Self-attention mechanism, FTMT offers an efficient architecture for encoding multimodal features. MCANet then further refines these embedding features, employing contrastive loss to mitigate potential domain discrepancies that might arise among different modalities.
The performance of UCFFormer was evaluated on two widely used benchmarks. UCFFormer achieved the state-of-the-art performance, surpassing the latest HAR methods.
Ablation studies confirmed the effectiveness of the ideas applied to UCFFormer. 
In conclusion, the UCFFormer presents a robust and adaptable technique for merging various data types to enhance HAR performance. Beyond HAR, this research holds promise for other applications where a joint representation of diverse data types is essential for task execution.