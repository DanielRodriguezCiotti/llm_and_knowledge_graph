\newcolumntype{D}{>{\centering\arraybackslash}p{9.5em}}
\newcolumntype{M}{>{\centering\arraybackslash}p{8.0em}}
\newcolumntype{A}{>{\centering\arraybackslash}p{4.5em}}
In this section, we evaluate the performance of the proposed UCFFormer on two widely used datasets: UTD-MHAD \cite{chen2015utd} and NTU RGB+D \cite{shahroudy2016ntu}. 

\subsection{Dataset} 

\textbf{UTD-MHAD:} The UTD-MHAD dataset comprises 27 human actions performed by eight subjects. It offers data in four distinct modalities: RGB video, depth video, skeletal joint positions, and inertial sensor signals.
For evaluating performance, we utilized the Top-1 accuracy metric. In this dataset, we adopted the cross-subject evaluation method widely used to evaluate the performance of HAR models \cite{chen2015utd}. This approach divides subjects into different groups for training versus testing to assess the model's ability to generalize to new, unseen subjects. Specifically, we used a dataset comprising 8 subjects for this experiment. Among them, 6 subjects were utilized for training, 1 subject for validation, and 1 subject for testing. 


\textbf{NTU RGB+D:} The NTU RGB+D dataset is an extensive collection of RGB video, depth video and skeleton data, consisting of 56,880 action samples performed by 40 subjects across 60 action classes.  In this dataset, two evaluation methods, namely cross-subject (CS) and cross-view (CV), have been widely employed \cite{shahroudy2016ntu}. For the CS evaluation method, 40 samples were randomly selected as training and testing groups, with the training IDs being 1, 2, 4, 5, 8, 9, 13, 14, 15, 16, 17, 18, 19, 25, 27, 28, 31, 34, 35, and 38. The remaining subjects were reserved for testing, resulting in 40,320 for training and 16,560 samples for testing. On the other hand, for CV evaluation, samples from cameras 2 and 3 were used for training, while samples from camera 1 were used for testing. The resulting sets comprised 37,920  for training and 18,960 samples for testing.

\subsection{Experimental Setup}

\subsubsection{Implementation Details}
We present the implementation details of UCFFormer. 
The performance of UCFFormer was evaluated across four different configurations. Based on the two types of proposed  factorized attention, UCFFormer is divided into UCFFormer-Sim and UCFFormer-Seq. Additionally, when using the Image Backbone ResNet50, it was denoted as R, and when using the Video Backbone TSM, it was denoted as T. Therefore, the four tested types were denoted as UCFFormer-Sim-R, UCFFormer-Seq-R, UCFFormer-Sim-T, and UCFFormer-Seq-T.

\textbf{UTD-MHAD:} 
In the case of UTD-MHAD,  we employed ResNet50 \cite{he2016deep} and TSM \cite{lin2019tsm} as RGB video backbone networks, while we used STGCN \cite{yu2018spatio} for skeleton data and DeepConvLSTM \cite{singh2020deep} for inertial data. 
For the RGB video data, ResNet50 was used to extract features from each of eight video frames of  $\textrm{height} \times \textrm{width} \times \textrm{channel} = 224 \times 224 \times 3$.  TSM was used to extract features from the eight video frames of $\mathrm{time} \times \mathrm{height} \times \mathrm{width} \times \mathrm{channel} = 8 \times 224 \times 224 \times 3$. The STGCN method extracted features from skeleton data of $\textrm{joint} \times \textrm{spatial position} = 20 \times 3$ using a window of size 15 over 8 time steps. For the inertial sensor data with $\textrm{Accelerometer} \times \textrm{Rotation} = 3 \times 3$, DeepConvLSTM was used with a window size of 100 over 8 time steps. The unified Transformer was configured with parameters, including a dimension of 512 and 8 attention heads.
%, along with a dimension of 64. 
Each layer was repeated 4 times for both the FTMT-Sim and FTMT-Seq modules.
During training, we employed stochastic gradient descent (SGD) as the optimizer for the entire network, utilizing a momentum of 0.9 and a learning rate of 0.0005. The training process for the FTMT-Sim and FTMT-Seq modules lasted for 16 hours and 300 epochs on the UTD-MHAD dataset. All training and testing were conducted on a single NVIDIA Geforce GTX 1080Ti GPU, with a batch size of 8.

\textbf{NTU RGB+D:} 
In the case of UTD-MHAD, we used TSM \cite{lin2019tsm} to extract the features from RGB video of size $\mathrm{time} \times \mathrm{height} \times \mathrm{width} \times \mathrm{channel} = 8 \times 224 \times 224 \times 3$. We also used STGCN to extract the features from skeleton data with dimension $\mathrm{joint} \times \mathrm{spatial}\,\,\mathrm{position} \times \mathrm{person} = 25 \times 3 \times 2$. We used a window of size 15 over 8 time steps. 
%The NTU RGB+D dataset captures both single-person actions and interactions between two individuals, whereas the UTD-MHAD dataset focuses solely on single-person actions.
Both the FTMT-Sim and FTMT-Seq modules were configured with a dimension of 512 and 16 attention heads.
%complemented by a dimension of 64. 
The Transformer encoder layer was repeated 4 times.
For optimization, we employed stochastic gradient descent (SGD) with a momentum of 0.9 and a learning rate of 0.003. The training duration was set to 480 hours and 70 epochs for the FTMT-Sim module, and 502 hours and 50 epochs for the FTMT-Seq module. Training is conducted with two NVIDIA Geforce GTX 1080Ti GPUs, with a batch size of 8.

\begin{table}
\begin{center}
\caption{Performance comparison on UTD-MHAD dataset: Top-1 Accuracy}
\label{tab1:UTD_MHAD}
\begin{adjustbox}{width=0.45\textwidth}
\begin{tabular}{c | c | c | c}
\Xhline{2.5\arrayrulewidth}
Method & Year & Modality &\begin{tabular}[c]{@{}c@{}}Accuracy\\(\%)\end{tabular} \\ \hline \hline
DCNN \cite{imran2016human} & 2016 & RGB, Depth & 91.20 \\
MCRL \cite{liu2018rgb} & 2018 & RGB, Depth & 93.02 \\
PoseMap \cite{liu2018recognizing} & 2018 & 3D Skeleton, Heatmap & 94.51 \\
HDM \cite{zhao2019bayesian} & 2019 & RGB, Skeleton & 92.8 \\ 
CC \cite{peng2019correlation} & 2019 & RGB, Inertial & 94.87 \\
Gimme$^{\cdot}$ Signals \cite{memmesheimer2020gimme} & 2020 & Skeleton, Inertial, Wifi & 93.33 \\ 
TPSMMs \cite{chen2020convnets} & 2020 & Skeleton & 93.26\\
LSTM-CNN \cite{zhu2020exploring}&2020 &Skeleton & 93.2 \\ 
HAMLET \cite{islam2020hamlet} & 2020 & RGB, Depth, Skeleton, Inertial & 95.12 \\
Fusion-GCN \cite{duhme2021fusion} & 2022 & RGB, Skeleton, Inertial & 94.42 \\
VSKD \cite{ni2022cross} & 2022 & RGB, Inertial & 96.97 \\
MuMu \cite{islam2022mumu} & 2022 & RGB, Depth, Skeleton, Inertial & 97.60 \\
MAVEN \cite{islam2022maven} & 2022 & RGB, Depth, Skeleton, Inertial & 97.81 \\
Multi-stream CNNs \cite{khezerlou2023multi} & 2023 & RGB, Depth, Skeleton, Inertial & 96.95 \\
\hline
 \textbf{UCFFormer-Sim-R(Ours)}& - & RGB, Skeleton, Inertial & \textbf{99.04} \\
 \textbf{UCFFormer-Sim-T(Ours)}& - & RGB, Skeleton, Inertial & \textbf{99.99} \\
 \textbf{UCFFormer-Seq-R(Ours)}& - & RGB, Skeleton, Inertial & \textbf{99.03} \\
 \textbf{UCFFormer-Seq-T(Ours)}& - & RGB, Skeleton, Inertial & \textbf{99.60} \\
\hline 
\Xhline{2.5\arrayrulewidth}
\end{tabular}   
\end{adjustbox}
\end{center}
\end{table}

\begin{table}
\begin{center}
\caption{Performance comparison on NTU RGB+D dataset: Cross-Subject (CS) and Cross-View (CV)}
\label{tab2:NTU_RGB+D}
\begin{adjustbox}{width=0.45\textwidth}
\begin{tabular}{c | c | c | c | c}
\Xhline{2.5\arrayrulewidth}
Method & Year & Modality & CS & CV \\ \hline \hline
PoseMap \cite{liu2018recognizing} & 2018 & RGB, Skeleton & 91.7 & 95.2 \\
TSMF \cite{bruce2021multimodal}  & 2020 & RGB, Skeleton & 92.5 & 97.4 \\
HAC \cite{davoodikakhki2020hierarchical} & 2020 & RGB, Skeleton & 95.66 & 98.79 \\
VPN \cite{das2020vpn} & 2020 & RGB, Skeleton & 95.5 & 98.0 \\
FUSION  \cite{de2020infrared} & 2020 & IR, Skeleton & 91.8 & 94.9 \\
InfoGCN \cite{chi2022infogcn}  & 2021 & Skeleton & 93.0 & 97.1 \\
PoseC3D \cite{duan2022revisiting} & 2021 & RGB, Skeleton & 97.0 & \textbf{99.6} \\
STAR-Transformer \cite{ahn2023star} & 2022 & RGB, Skeleton & 92.0 & 96.5 \\
\hline
\textbf{UCFFormer-Sim-T(Ours)} & - & RGB, Skeleton & \textbf{97.1} &  99.3 \\
\textbf{UCFFormer-Seq-T(Ours)} & - & RGB, Skeleton & 95.0 & 98.0 \\

\Xhline{2.5\arrayrulewidth}
\end{tabular}   
\end{adjustbox}
\end{center}
\end{table}

\begin{figure*}[tbh]
    \centering
    \captionsetup[subfloat]{labelfont=tiny,textfont=tiny}
    \subfloat[RGB]{ 
        \includegraphics[width=.13\textwidth]{Figure/Conf_RGB.pdf}}
    \quad
    \subfloat[Inertial]{ 
        \includegraphics[width=.13\textwidth]{Figure/Conf_Inertial.pdf}}
    \quad
    \subfloat[Skeleton]{ 
        \includegraphics[width=.13\textwidth]{Figure/Conf_Skel.pdf}}
    \quad
    \subfloat[RGB+Inertial]{ 
        \includegraphics[width=.13\textwidth]{Figure/Conf_RGB_Iner.pdf}}
    \quad
    \subfloat[RGB+Skeleton]{ 
        \includegraphics[width=.13\textwidth]{Figure/Conf_RGB_Skel.pdf}}
    \quad
    \subfloat[RGB+Inertial+Skeleton]{ 
        \includegraphics[width=.13\textwidth]{Figure/Conf_all.pdf}}
    \caption{\textbf{Confusion matrices for demonstrating the effect of multimodal fusion}. The evaluation is conducted using UCFFormer-Sim-R architecture on the UTD-MHAD dataset. 
    %Confusion matrices were examined for prediction scores in each class when the modality was unimodal, when RGB and inertial were fused, when RGB and skeleton were fused, and finally when RGB, inertial, and skeleton were fused. The evaluation was focused on RGB, which is the most commonly used visual data.
    }
    \label{fig4:confusion_matrix}
\end{figure*}

\subsection{Performance Comparison}
The performance of UCFFormer was evaluated in comparison with conventional HAR methods. 
Table \ref{tab1:UTD_MHAD} presents the Top 1 accuracy results of the HAR models of interest evaluated on the UTD-MHAD dataset. 
We note that all UCFFormer variants achieve significant performance gains over the other HAR methods. In particular, UCFFormer-Sim-T set a new state-of-the-art performance with an impressive Top 1 accuracy of 99.99\%. It outperforms the latest MAVEN model \cite{islam2022maven} by a significant margin of 2.18\%.  These results show the effectiveness of UCFFormer in handling the multimodal data. We also observe that both FTMT-Sim and FTMT-Seq achieve comparable performance. 

\begin{table}
\begin{center}
\caption{Performance impact of multimodal fusion evaluated  on the UTD-MHAD Dataset}
\label{tab3:Modality_Test}
\begin{adjustbox}{width=0.45\textwidth}
\begin{tabular}{c||A|A|A||c}
\Xhline{2.5\arrayrulewidth}
 \multirow{2}{*}{Method} & \multicolumn{3}{c||}{Modalities} &\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Accuracy\\(\%)\end{tabular}}\\\cline{2-4}
 & RGB & Inertial & Skeleton & \\
\hline\hline
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}
Unimodal\end{tabular}} & \checkmark & & & 87.54\\
& & \checkmark & & 86.00   \\
& & & \checkmark &  94.72 \\ \hline
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}
Multimodal\end{tabular}} & \checkmark & \checkmark & & 91.35 \\
& \checkmark &  & \checkmark & 96.15 \\
& \checkmark & \checkmark & \checkmark & 99.04 \\
\Xhline{2.5\arrayrulewidth}
\end{tabular}   
\end{adjustbox}
\end{center}
\end{table}




Table \ref{tab2:NTU_RGB+D} presents the performance of UCFFormer evaluated on the NTU RGB+D dataset \cite{shahroudy2016ntu}.
UCFFormer exhibits a performance that is on par with the current state-of-the-art method, PoseC3D \cite{duan2022revisiting}. Notably, UCFFormer offers substantial performance gains over other HAR models, underscoring its competitive performance in accurately recognizing human activities. It is also worth mentioning that FTMT-Sim achieves slightly better performance than FTMT-Seq on NTU RGB+D dataset. 

\subsection{Performance Behavior}
In this section, we present a comprehensive analysis of performance behavior of our UCFFormer.

\subsubsection{Effect of Multimodal Fusion}
Table \ref{tab3:Modality_Test} investigates performance gains achieved by multimodal fusion using UCFFormer-Sim-R on the UTD-MHAD dataset. 
While the original UCFFormer combined three modalities including RGB video, skeleton, and inertial data, Table \ref{tab3:Modality_Test} provides the performance achieved when only a single modality is used or only two modalities are combined. When a single modality was used, the embedding vectors were encoded solely through Temporal Attention, and then the flattened embedding vectors were used for classification. We observe that the performance of UCFFormer considerably decreases when employing a single modality or combining two modalities. When we use RGB, inertial, and skeleton data individually, the skeleton data yields the highest performance 94.72\%, which is 4.32\% lower than the accuracy of three modalities. When only two modalities are combined, the highest achieved accuracy is 96.15\%, exhibiting a 2.89\% reduction compared to the accuracy of the original design.
Figure \ref{fig4:confusion_matrix} presents the confusion matrices obtained with different combinations of modalities used for UCFFormer-Sim-R. It confirms that the inclusion of an additional modality leads to substantial improvements in classification accuracy.

\begin{figure}[!t]
    \centering
    \captionsetup[subfloat]{labelfont=footnotesize,textfont=footnotesize}
    \subfloat[The t-SNE plot with contrastive learning]{ 
        \includegraphics[width=.4\textwidth]{Figure/tsne_before.png}}
    \quad
    \subfloat[The t-SNE plot without constrative learning]{ 
        \includegraphics[width=.4\textwidth]{Figure/tsn_after.png}}
         \vspace{2mm}
    \caption{\textbf{t-SNE visualization of multimodal feature vectors.} We present the t-SNE plots of the multimodal feature vectors extracted from the RGB video (represented in red), skeleton (represented in blue), and inertial (represented in green) modalities. We compare the feature distributions with contrastive learning versus without contrastive learning.
    %We indicate the distribution of each of the feature vectors has changed with and without MCANet in (a) and (b).
    }
    \label{fig6:tsne}
\end{figure}

\begin{figure}[thb]
    \centering
    \captionsetup[subfloat]{labelfont=footnotesize,textfont=footnotesize}
    \subfloat[Performance versus \# of Transformer layers]{ 
        \includegraphics[width=.33\textwidth]{Figure/layers.pdf}}
        \vspace{5mm}
     \subfloat[Performance versus \# of multi-attention heads]{ 
        \includegraphics[width=.33\textwidth]{Figure/heads.pdf}}
     \vspace{5mm}
    \subfloat[Performance versus embedding size]{ 
        \includegraphics[width=.33\textwidth]{Figure/dimensions.pdf}}
        \vspace{2mm}
    \caption{Performance of UCFFormer for different hyperparameter values. }
    \label{fig5:transformer_parameter}
\end{figure}

\begin{table}[tbh]
\begin{center}
\caption{Ablation study on UTD-MHAD dataset (Top-1 Accuracy)}
\label{tab4:Module_Test}
\begin{adjustbox}{width=0.35\textwidth}
\begin{tabular}{A|A|A||c}
\Xhline{2.5\arrayrulewidth}
 \multicolumn{3}{c||}{Settings} &\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Accuracy\\(\%)\end{tabular}}\\\cline{1-3}
 {\begin{tabular}[c]{@{}c@{}}FTMT-Sim\end{tabular}} & {\begin{tabular}[c]{@{}c@{}}FTMT-Fac\end{tabular}} & MCANet  & \\
\hline\hline
& & & 92.03 \\

\checkmark & & & 97.87\\ 
 & \checkmark & &  97.13 \\
 &  & \checkmark & 96.15 \\
\hline
 \checkmark & & \checkmark & 99.04 \\
 & \checkmark & \checkmark & 99.03 \\
\Xhline{2.5\arrayrulewidth}
\end{tabular}   
\end{adjustbox}
\end{center}
\end{table}

\subsubsection{Effect of Contrastive Learning}

Figure \ref{fig6:tsne} displays a t-SNE analysis that represents features in a reduced-dimensional space. We compared the feature distributions when using contrastive learning and when not using it.
We observed that with the application of contrastive learning, the distribution gap in features across different modalities is notably reduced as intended. 



\subsubsection{Performance versus Hyperparameters}
In Figure \ref{fig5:transformer_parameter}, we evaluate the performance of the UCFFormer when applying different hyperparameter values including  the number of Transformer layers, embedding size, and attention heads.   The default values of the number of layers, the embedding size, and the number of attention heads are set to 4, 512, and 8, respectively.

Figure \ref{fig5:transformer_parameter} (a) presents the performance versus the number of Transformer layers for both FTMT-Sim and FTMT-Seq. Our observations reveal that the peak classification accuracy is attained with 4 layers, and increasing the depth beyond this point adversely impacts performance. 
Figure \ref{fig5:transformer_parameter} (b) provides the performance versus the number of multi-heads. Performance improvement diminishes as the number of heads is above 8. Figure \ref{fig5:transformer_parameter} (c) provides the performance of UCFFormer versus embedding size. The performance dramatically improves as embedding size increases from 16 up to 64 but plateaus beyond 64, reaching its peak at 512. For all cases, FTMT-Sim consistently outperforms FTMT-Seq. 

\subsubsection{Ablation Studies} 
Table \ref{tab4:Module_Test} presents an ablation study presenting the contributions made by each component of UCFFormer to the overall performance.  We employed a ResNet-50 backbone and evaluated the performance using the Top-1 accuracy metric on the UTD-MHAD dataset. The baseline method neither includes FTMT nor MCANet; it directly inputs the embedding vectors into the Flatten + MLP module.  Table \ref{tab4:Module_Test} demonstrates that the baseline attains an accuracy of merely 92.03\%, which falls short by 7\% in comparison to UCFFormer-Sim-R. Upon integrating FTMT-Sim with the baseline, there is a notable accuracy enhancement of 5.84\%. The inclusion of FTMT-Seq results in a performance improvement of 5.1\%. This demonstrates that the unified Transformer enhances the action semantics through joint time-modality self-attention. Adding MCANet further improves the accuracy by 1.17\% for FTMT-Sim and by 1.9\% for FTMT-Seq. Note that in the absence of FTMT, the utilization of MCANet alone yields a notable 4.12\% improvement in accuracy compared to the baseline, which demonstrates the efficacy of MCANet.   




\begin{table}[t]
    \begin{center}
    \caption{Comparison of computational complexity between factorized Self-Attention versus full self-attention on the UTD-MHAD dataset}
    \label{tab5:Comp_Attention}
    \resizebox{\columnwidth}{!}{%
        \begin{tabular}{ c || c | c | A | A | A | A }
        \Xhline{2.5\arrayrulewidth}
 \multirow{2}{*}{Module} & \multirow{2}{*}{\begin{tabular}{c} \# of \\ layers \end{tabular}} & \multirow{2}{*}{Accuracy (\%)} & \multicolumn{2}{c|}{Whole Network} & \multicolumn{2}{c}{ Transformer Encoder} \\\cline{4-7}
 & & & Params(M) & GFLOPs & Params(M)  & MFLOPs\\
        \hline \hline
        \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Factorized \\Self-Attention \end{tabular}} & 2 & 99.04 & 33.72 & 66.96 & 7.69 & 122.8\\ \cline{2-7}
          & 3 & 99.04 & 36.87 & 67.00 & 10.84 & 173.16 \\ \cline{2-7}
          & 4 & 99.04 & 40.02 & 67.06 & 13.99 & 223.52 \\ \cline{2-7}
          & 5 & 98.07 & 43.17 & 67.06 & 17.14 & 273.88 \\
        \hline 
        \multirow{4}{*}{Self-Attention}& 2 & 97.15 & 43.17 & 67.1 & 17.15 & 273.94 \\ \cline{2-7}
          & 3 & 96.12 & 51.05 & 67.22 & 25.03 & 399.88 \\ \cline{2-7}
          & 4 & 95.04 & 58.93 & 67.36 & 32.91 & 525.82\\ \cline{2-7}
          & 5 & 87.75 & 66.81 & 67.48 & 40.79 & 705.74\\
        \Xhline{2.5\arrayrulewidth}
        \end{tabular}
        }
    \end{center}
\end{table}

Table \ref{tab5:Comp_Attention} compares the performance and computational complexity between the full-dimenional self-attention versus the factorized self-attention of FTMT-Sim.  
Table \ref{tab5:Comp_Attention} highlights the computational advantage of that Factorized Self-Attention over the conventional Self-Attention mechanism. For each added layer, Factorized Self-Attention adds approximately 50 million floating-point operations per second (MFLOPs), while the conventional Self-Attention sees an increase of 130 MFLOPs. 
Table \ref{tab5:Comp_Attention} also shows a substantial reduction in the number of parameters by adopting the Factorized Self-Attention. 
In spite of significant reduction in the memory usage and computation time, the factorized self-attention achieves slightly better accuracy than the full self-attention. 