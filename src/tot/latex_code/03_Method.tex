\begin{figure*}[tbh]
    \centering
    \includegraphics[width=0.99\textwidth]{Figure/overall.pdf}
    \caption{\textbf{Overall Architecture} : The proposed UCFFormer first represents each raw multimodal data in a shared feature space. FTMT encodes these embedding features, capturing their dependencies within the time-modality domain using the Unified Transformer. Subsequently, MCANet refines these embeddings by aligning them across modalities through contrastive learning. These enhanced embeddings are then aggregated, leading to the final classification results.}
    \label{fig1:Overall_Architecture}
\end{figure*}

\subsection{Overview}
Figure \ref{fig1:Overall_Architecture} depicts the overall structure of the proposed UCFFormer. The proposed UCFFormer fuses the information from $N$ multimodal sensors. First, $N$ separate backbone networks are employed to extract sequential feature vectors of length $T$ from each modality. The feature vectors are linearly projected to produce the $NT$ embedding vectors of the same size. The projected embedding vectors serve as basic semantic elements represented in time and modality domain.
Factorized Time-Modality Transformer (FTMT) encodes the embedding vectors using a unified Transformer architecture. The unified Transformer models both intra-modality and inter-modality interactions simultaneously to produce the updated embedding vectors. To facilitate effective interaction modeling, FTMT employs a {\it factorized self-attention mechanism} that conducts the encoding process separately in temporal and modality domains.

Next, the Multimodal Contrastive Alignment Network (MCANet) combines the features generated by FTMT.
Among $N$ multimodal sensors, we designate one as main modality sensor and the others as $N-1$ sub-modality sensors. MCANet boosts the effect of feature fusion by aligning the sub-modality features with those of the main modality through contrastive learning. Finally, the combined features are passed through a multi-layer perceptron (MLP) layer followed by a softmax layer to generate the final classification result.

\subsection{Setup and notations}
We make the assumption that $N$ sensor data are temporally synchronized. To achieve this, we resample the data such that their sampling rates are all identical. Our model takes $T$ consecutive samples of all modality data in a sequential manner. We denote $T$ signal samples acquired from $N$ modality sensors as $x_{1}[1:T], x_{2}[1:T],..., x_{N}[1:T]$, where $x_{n}[1:T]=\{x_{n}[1],..., x_{n}[T]\}$. The dimension of each sample is different for each modality. 

\subsection{Multimodal Feature Extractor}
Different backbone networks are employed to extract feature vectors from each modality data. Suppose that the feature vectors $f_{n}[1:T]$ are obtained from the input data $x_{n}[1:T]$.
Then, we apply linear projection to map the feature embedding vectors into the embedding vectors of common size $d$, i.e.,
\begin{equation}
    \begin{split}
     Z_{1}[1:T] &= W_{1}\cdot f_{1}[1:T] \\
    %Z_{2}[1:T] &= W_{2}\cdot f_{2}[1:T] \\
   & \vdots \\
    Z_{N}[1:T] &= W_{N}\cdot f_{N}[1:T].
    \end{split}
  \end{equation}
These $NT$ embedding vectors form the basis for finding the joint representation of multimodal data.

\subsection{Factorized Time-Modality Transformer Encoder}
FTMT encoder encodes the embedding vectors capturing their high-level inter-dependencies. 
To this goal, we utilize the unified Transformer, which has been utilized for vision-language multimodal data modeling  \cite{zhou2020unified}.  The unified Transformer leverages Transformer Self-attention to encode the embedding vectors across different modalities and time steps.

However, given $NT$ embedding vectors, FTMT requires the computational complexity of $\mathcal{O}(N^2 T^2)$ and high capacity network for learning all possible pairwise relations among $NT$ elements. To cope with this issue, we employ the Factorized Time-Modality Self-attention, which conducts self-attention across time and modality domains  independently. We present two distinct versions of factorized self-attention, which are differentiated by their respective arrangements of time-domain and modality-domain self-attention operations.


\subsubsection{Module 1: Simultaneous Time-Modality Factorization }
The Simultaneous Time-Modality Factorization (FTMT-Sim) approach conducts self-attention operations concurrently in the time and modality domains and merges the encoded features in the final attention layer. Figure \ref{fig2:FSC} (a) depicts the structure of FTMT-Sim. For each time step $t$, the embedding vectors across different modalities are packed into a matrix $Z[t]=[Z_{1}[t],...,Z_{N}[t]]$. Similarly, the embedding vectors across time steps are packed into a matrix $Z_{n}=[Z_{n}[1],...,Z_{n}[T]]$. Then, self-attention is applied independently to each axis in parallel. First, self attention across modalities is performed as
\begin{equation}
    \begin{split}
    Z^{(l+1)}[t] &= \mathrm{ModalityAttention}(Z^{(l)}[t]) \\ 
    &= \mathrm{Softmax}\left( \frac{Q^{(l)}[t]((K^{(l)}[t])^{T}V^{(l)}[t])}{\sqrt{d_{k}}} \right),
    %&= \mathrm{Concat}(Y^{(l)}_{1}[t],\dots,Y^{(l)}_{h}[t]))W^{(l)}_{t,o} 
    \end{split}    
\end{equation}
where 
\begin{equation}
    \begin{split}
    Q^{(l)}[t] &= Z^{(l)}[t] \cdot W^{(l)}_{q}  \\
    K^{(l)}[t] &= Z^{(l)}[t] \cdot W^{(l)}_{k}  \\
    V^{(l)}[t] &= Z^{(l)}[t] \cdot W^{(l)}_{v},
    \end{split}
\end{equation}
$l \in [1,L] $ is the index of attention layer and $W^{(l)}_{q} \in \mathbb{R}^{d \times d_{k}}$, $W^{(l)}_{k} \in \mathbb{R}^{d \times d_{k}}$, and $W^{(l)}_{v} \in \mathbb{R}^{d \times d_{v}}$ are the linear weights, where $d_{k}$ and $d_{v}$ represent the dimensions of the keys and values, respectively. For brevity, we exclude the notation for multi-head attention.
In our implementation, both $d_{k}$ and $d_{v}$ are set to 64.

Next, the positioning encoding is applied to the embedding vectors in $Z_{n}$ \cite{vaswani2017attention} for each modality index $n$.  Then, self-attention across time steps is performed as
\begin{equation}
    \begin{split}
    Z^{(l+1)}_{n} &= \mathrm{TemporalAttention}(Z^{(l)}_{n}) \\ 
    &=  \mathrm{Softmax}\left( \frac{Q_n^{(l)}((K_n^{(l)})^{T}V_n^{(l)})}{\sqrt{d_{k}}} \right)\,
    \end{split}    
\end{equation}
where 
\begin{equation}
    \begin{split}
    Q_n^{(l)} &= Z^{(l)}_{n} \cdot V^{(l)}_{q} \\
    K_n^{(l)} &= Z^{(l)}_{n} \cdot V^{(l)}_{k} \\
     V_n^{(l)} &= Z^{(l)}_{n} \cdot V^{(l)}_{v},
     \end{split}
\end{equation}
 $W^{(l)}_{n,q} \in \mathbb{R}^{d \times d_{k}}$, $W^{(l)}_{n,k} \in \mathbb{R}^{d \times d_{k}}$, and $W^{(l)}_{n,v} \in \mathbb{R}^{d \times d_{v}}$ are the linear weights.

After $L$ attention layers, the attention values $Z^{(L)}[1],..., Z^{(L)}[T]$ and $Z^{(L)}_{1},...,Z^{(L)}_{N}$ are arranged, concatenated and linearly projected, resulting in the generation of the final $NT$ features $C_{1}[1:T],...,C_{N}[1:T]$. 
Note that the skip connection, originating from the initial values $Z^{(0)}_{n}$ and $Z^{(0)}[t]$, is incorporated during the process of generating the final features. 

\begin{figure*}[tbh]
    \centering
    \captionsetup[subfloat]{labelfont=footnotesize,textfont=footnotesize}
    \subfloat[\textbf{Simultaneous Time-Modality Factorization (FTMT-Sim)}]{\includegraphics[width=0.50\textwidth]{Figure/Sim_Fac.pdf}}
    \quad\quad
    \subfloat[\textbf{Sequential Time-Modality Factorization (FTMT-Seq)}]{\includegraphics[width=0.3\textwidth]{Figure/Seq_Fac.pdf}}
    \caption{\textbf{Factorized Time-Modality Self-attention} : (a) FTMT-Sim processes the embedding features in both time and modality domains concurrently, (b) FTMT-Seq alternates the encoding process between the time-domain and the modality-domain in a sequential manner. }
    \label{fig2:FSC}
\end{figure*}

\subsubsection{Module 2: Sequential Time-Modality Factorization (FTMT-Seq)}
Unlike FTMT-Sim, the Sequential Time-Modality Factorization (FTMT-Seq) approach applies the self-attention operations in time axis and modality axis one by one. First, after positional encoding is performed, the time-domain self-attention operation is performed  as
\begin{equation}
    Y^{(l)}_{n} = \mathrm{TemporalAttention}(Z_n^{(l)})
\end{equation}
Then, the output $Y^{(l)}_{1},...,Y^{(l)}_{N}$ are rearranged to $Y^{(l)}[1],...,Y^{(l)}[T]$.
%where the embedding vectors across different modalities are packed into a matrix $Y^{(l)}[t]=[Y^{(l)}_{1}[t],...,Y^{(l)}_{N}[t]]$.  
Then, the modality-domain self-attention follows 
\begin{equation}
    Z^{(l+1)}[t] =  \mathrm{ModalityAttention}(Y^{(l)}[t]). 
\end{equation}
After passing through $L$ attention layers and skip connection from the input, FTMT-Seq produces the final features  $C_{1}[1:T],...,C_{N}[1:T]$.

\subsection{Multimodal Constrastive Alignment Network}
Utilizing the output of FTMT, $C_{1}[1:T],...,C_{N}[1:T]$, MCANet employs Weighted Multimodal Feature Fusion that combines the multi-modal features.  In our framework, we let $C_{1}[1:T]$ be the main modality features and the rest be the sub-modality features.
The primary goal of this approach is to selectively aggregate the information that is pertinent to the main modality data from the sub-modality data. 
For each time step $t$, Weighted Multimodal Feature Fusion combines the multimodal features as
\begin{align}
        C_{agg}[t] &= C_{1}[t] + \sum_{n=2}^{N} \mathrm{sigmoid}(\mathrm{sim}(C_{1}[t], C_{n}[t])) C_{n}[t],   \end{align}
where $\mathrm{sim}(A,B)$ denotes the cosine similarity measure
\begin{align}
    \begin{split}
    \mathrm{sim}(A,B) = \frac{A^{T} \cdot B}{\lVert A \rVert_2 \lVert B \rVert_2}. 
    \end{split}
\end{align}
Note that the sub-modality features $C_{n}[t]$ $n \neq 1$ are weighted according to their similarity with the main modality features  $\mathrm{sigmoid}(\mathrm{sim}(C_{1}[t], C_{n}[t]))$. 
%the similarity measure $\mathrm{sigmoid}(\mathrm{sim}(C_{1}[t], C_{n}[t]))$ determines the combining weight for the modality $C_{n}[t]$.
Additionally, we employ a contrastive learning framework to reduce feature discrepancy across different modalities. By minimizing a contrastive loss that captures the dis-similarities between the different modalities during training, we can make our weighted feature aggregation more effective. Details on the contrastive loss will be discussed in the following subsection.

% In addition, we minimize the feature differences across various modalities via a contrastive learning framework. More specifically, the similarity between the embedding features is maximized by employing a contrastive loss during the training phase. Such reduced domain gap makes our weighted feature aggregation more effective.
%  The details of contrastive loss will be presented in the subsequent subsection.

Finally, MCANet flattens the matrix $[{C}_{agg}[1], ..., {C}_{agg}[T]] $ into a single-dimensional vector. The flattened vector passes through the MLP followed by softmax layer, generating the final classification scores $O\in \mathbb{R}^{n_{cls}}$, where $n_{cls}$ is the number of action classes. Each element in the vector $O$ represents the probability score assigned to a specific action class. By analyzing these probabilities, the model can determine the most likely class for a given input.

\subsection{Loss Function}
As depicted in Figure \ref{fig1:Overall_Architecture}, the loss function used to train our entire network consists of the contrastive loss term $\mathcal{L}_{contrast}$ and the cross entropy loss term $\mathcal{L}_{cls}$, i.e.,  
\begin{equation}
\mathcal{L} = \mathcal{L}_{cls} + \alpha \mathcal{L}_{contrast},
\end{equation}
where $\alpha$ is the regularization parameter. In our approach, we set $\alpha$ to 0.2.
%indicating that the differential loss contributes to 20\% of the overall loss during training.

The contrastive loss term $\mathcal{L}_{contrast}$ aims to encourage the alignment of features among different modalities. It quantifies the dissimilarity or disparity between the main modality and the $N-1$ sub-modalities, i.e.,  
\begin{equation}
\label{eq:diff_loss}
    \mathcal{L}_{contrast} = - \sum_{n=2}^{N} \log \left(\frac{1}{T}\sum_{t=1}^{T}\mathrm{sim}(C_{1}[t],C_{n}[t])\right)
\end{equation}
By using the contrastive loss term, the model can generate semantically consistent representations. 
%Such alignment is effective for tasks requiring consolidation of information via feature fusion from various sources.

The cross entropy loss term $\mathcal{L}_{cls}$ is given by
\begin{equation}
\mathcal{L}_{cls} = - \sum_{i = 1}^{n_{cls}}{T_{i}\log(O_{i})},
\end{equation}
where $O_i$ is the $i$th element of $O$ and $T_i$ is the $i$th element of ground truth one hot vector.  

\section{Implementation of Multimodal HAR System}
%Our UCFFormer is capable of fusing data from any type and number of sensors for HAR. 
In this section, we present design variants of UCFFormer, specifically tailored for the sensor configurations provided in two datasets: UTD-MHAD \cite{chen2015utd} and NTU RGB+D \cite{shahroudy2016ntu}.

\subsection{Implementation of UCFFormer on UTD-MHAD dataset}
UCFFormer is designed to fuse three different modalities including RGB video,  skeleton data and inertial sensor signals provided by UTD-MHAD dataset.  RGB video consists of length-$T$ sequence of video frames, i.e.,  $x_{1}[1:T] \in \mathbb{R}^{T \times H \times W}$, where $H$ and $W$ are the height and width of a video frame. The skeleton data  consists of 3D coordinates of $J$ joints, i.e., $x_{2}[1:T] \in \mathbb{R}^{T \times J \times 3}$, where $J$ is the number of joints. The data from the inertial sensors is divided into segments, with each segment being processed individually. The inertial sensor data is also processed in a similarly way, yielding vectors of the dimension $S$,  $x_{3}[1:T] \in \mathbb{R}^{T \times S}$.

We tried two different camera backbones; the ResNet50 model and  the Temporal Shift Module (TSM) \cite{lin2019tsm}. ResNet50 was used to to encode every frame of the RGB video while TSM was used to encode sequential RGB video.  We specifically selected ResNet50 to ensure fair comparison with several latest methods \cite{imran2016human,liu2018rgb,islam2020hamlet,islam2022mumu} that utilized ResNet50 on the UTD-MHAD dataset. We also tried a stronger backbone, TSM at the cost of higher computational complexity.  These backbone networks take video frames $x_1[1:T]$ as input and produces the sequence of feature maps $f_1[1:T]$. We encoded the skeleton data $x_2[1:T]$ using a Spatio-Temporal Graph Convolutional Network (STGCN) \cite{yu2018spatio}. Lastly, a DeepConvLSTM \cite{singh2020deep} was employed to encode the $T$ segments of inertial sensor data $x_{3}[1:T]$ and produce the feature vectors $f_{3}[1:T]$. 
The remaining steps follow the procedure described previously. 
In contrastive learning setup, RGB video acted as a primary modality, while the remaining data sources served as sub-modalities.

\subsection{Implementation of UCFFormer on NTU RGB+D dataset}
We consider two modalities: RGB images and skeleton data on NTU RGB+D dataset. The data preprocessing step was performed similarly as in UTD-MHAD dataset. To encode the RGB image sequence, we employ TSM as a video backbone network. 
The samples of NTU RGB+D dataset capture both individual actions and interactions among multiple individuals. 
From this perspective, we processed skeleton data for each of $P$ persons and constructed the tensor, 
$x_{2}[t] \in \mathbb{R}^{T \times J \times P \times 3}$.
The skeleton data was processed using the same backbone architecture used for the UTD-MHAD dataset. In contrastive learning setup, RGB video was selected as a primary modality.
