\subsection{Multimodal Fusion Methods for HAR}
Early studies in multimodal fusion presented a late fusion approach which combined classification results obtained from each modality to obtain a final prediction. 
%In Dawar's (2018) studies on data and action recognition 
In \cite{dawar2018action,dawar2018data}, Dawar et al. encoded RGB images using CNN and inertial sensor data using Long Short Term Memory (LSTM) and combined the resulting classification outcomes using the weights derived from the decision fusion scores. In \cite{imran2020evaluating}, 1D CNN, 2D CNN, and Recurrent Neural Network (RNN) were employed to predict action class based on Gyroscope data, RGB data, and human joint pose data, respectively and the classification results were combined. 
In \cite{zou2019wifi}, WiVi utilized a CNN backbone to represent WiFi signal and a C3D backbone \cite{tran2015learning} to process RGB data. These were then integrated at the decision level through an ensemble fusion model.

Several studies have explored achieving information fusion at the intermediate feature level.  
In \cite{long2018multimodal}, a Keyless Attention method was presented to aggregate the features extracted from multimodal data.
HAMLET \cite{islam2020hamlet} employed Hierarchical Multimodal Self-attention to obtain action-related spatio-temporal features. MuMu \cite{islam2022mumu} was trained to conduct multiple tasks, i.e.,  the target task of HAR and an auxiliary task of human action grouping. The auxiliary task assists the target task in extracting appropriate multimodal representations.
VSKD \cite{ni2022cross} utilized ResNet18 as a teacher network for video data and employed multi-scale TRN \cite{zhou2018temporal} with BN-Inception as a student network for inertial data.  Distance and Angle-wise Semantic Knowledge (DASK) loss was proposed to account for the modality differences between the vision and sensor domains.
MAVEN \cite{islam2022maven}  employed the feature encoders to generate modality-specific spatial features that were subsequently stored in memory banks. The memory banks were used to capture long-term spatiotemporal feature relationships. 

Our UCFFormer is different from aforementioned methods in that a joint representation of multimodal data is found through the time-modality factorized self-attention of the unified Transformer and using contrastive learning framework.

\subsection{Contrastive Learning}
Contrastive learning is a type of the self-supervised learning tasks that provides a means of understanding the differences between representations. This stems from the work of Bromley et al. who introduced the concept of a Siamese Network, consisting of two identical networks that share weights for metric learning \cite{bromley1993signature}. Specifically, contrastive learning examines which pairs of data points are similar and different to learn high-level data features before performing classification or segmentation tasks.

Early studies used a contrastive learning framework to learn invariant mappings for recognition using contrastive pair loss in discrimination models \cite{chopra2005learning,hadsell2006dimensionality}. Inspired by triplet loss, recent studies applied feature extraction methods that minimize the distance between representations of similar positive pairs and maximize the distance between representations of different negative pairs \cite{weinberger2009distance,collobert2008unified,chechik2010large}.

Recently, contrastive learning has widely been used for various image classification tasks.
Momentum Contrast (MoCo) \cite{he2020momentum,chen2020improved} used a momentum-updated encoder to produce representations of negative samples, providing a large and consistently maintained set of negatives for contrastive loss calculations.
SimCLR \cite{chen2020simple,chen2020big} learned representations by maximizing the similarity between different augmentations of the same data sample while minimizing the similarity between different samples.
Bootstrap Your Own Latent (BYOL) \cite{grill2020bootstrap} achieved self-supervised image representation learning without using negative samples by creating two augmented views of the same image.

\subsection{Unified Transformer for Multimodal Task}
Transformer architectures have achieved significant success in machine learning tasks, including natural language processing \cite{kenton2019bert,brown2020language} and computer vision \cite{dosovitskiy2020image}. However, they have mainly been limited to tasks within a single domain or specific multimodal domains. To overcome this challenge, a Unified Transformer model \cite{hu2021unit} has been proposed as a foundation model for multimodal data. 
The Unified Transformer consists of transform encoders jointly encoding multimodal inputs, and a transform decoder over the encoded input modalities, followed by task-specific outputs applied to the decoder hidden states to make final predictions for each task. The Unified Transformer handles multi-tasking and multimodal in a single model with fewer parameters, moving toward general intelligence. 

The Unified Transformer has been used for a variety of tasks that involve multimodal data.
UFO \cite{wang2021ufo} used a single transformer network for multimodal data and implemented multi-task learning during vision-language pre-training. The same transformer network was used as an image encoder, as a text encoder, or as a fusion network in the different pre-training tasks.
UniFormer \cite{li2021uniformer} unified 3D convolution and spatio-temporal self-attention to generate a transformer embedding vector. UniFormer's relation aggregator handles both spatio-temporal redundancy and dependency by learning local and global token correlations in shallow and deep layers, respectively.
UniTranSeR \cite{ma2022unitranser} embedded the multimodal features into a unified Transformer semantic space to prompt inter-modal interactions, and then employed a Feature Alignment and Intention Reasoning layer to perform cross-modal entity alignment.