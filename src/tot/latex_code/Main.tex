\documentclass[letterpaper,journal]{IEEEtran}
% \documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
% subfigure
\usepackage{subfig}
%% Formula size
\usepackage{nccmath, graphicx}
%% Table Package
\usepackage{multirow}
\usepackage{makecell}
\usepackage{adjustbox}
\usepackage{hhline}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\usepackage{balance}
%\usepackage{lmodern}
% updated with editorial comments 8/9/2021

\begin{document}

\title{Unified Contrastive Fusion Transformer for Multimodal Human Action Recognition }
\author{Kyoung Ok Yang, Junho Koh and Jun Won Choi$^*$
        % <-this % stops a space
% \thanks{Manuscript received September 00, 2023.}%
\thanks{* Corresponding author: Jun Won Choi.}% <-this % stops a space
\thanks{K. Yang is with Department of Artificial Intelligence, Hanyang University, 222, Wangsimni-ro, Seongdong-gu, Seoul, 04763, Korea (email: koyang@spa.hanyang.ac.kr)}%
\thanks{J. Koh is with Department of Electrical Engineering, Hanyang University, 222, Wangsimni-ro, Seongdong-gu, Seoul, 04763, Korea (email: jhkoh@spa.hanyang.ac.kr)}%
\thanks{J. W. Choi is with Department of Electrical Engineering and Graduate School of Artificial Intelligence, Hanyang University, 222, Wangsimni-ro, Seongdong-gu, Seoul, 04763, Korea (email: junwchoi@hanyang.ac.kr)}
}

% The paper headers
%\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
\markboth{}%
{Unified Contrastive Fusion Transformer for Multimodal Human Action Recognition}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
Various types of sensors have been considered to develop human action recognition (HAR) models. Robust HAR performance can be achieved by fusing multimodal data acquired by different sensors. In this paper, we introduce a new multimodal fusion architecture, referred to as Unified Contrastive Fusion Transformer (UCFFormer) designed to integrate data with diverse distributions to enhance HAR performance. Based on the embedding features extracted from each modality, UCFFormer employs the Unified Transformer to capture the inter-dependency among embeddings in both time and modality domains. We present the Factorized Time-Modality Attention to perform self-attention efficiently for the Unified Transformer. UCFFormer also incorporates contrastive learning to reduce the discrepancy in feature distributions across various modalities, thus generating semantically aligned features for information fusion. Performance evaluation conducted on two popular datasets, UTD-MHAD and NTU RGB+D, demonstrates that UCFFormer achieves state-of-the-art performance, outperforming competing methods by considerable margins. 
\end{abstract}

\begin{IEEEkeywords}
Human Action Recognition, Sensor Fusion, Multimodal Fusion, Unified Transformer, Factorized Attention, Contrastive Learning, UTD-MHAD, NTU RGB+D
\end{IEEEkeywords}

%%%%%%%%%%% 1. Introduction %%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}\label{sec1}
\input{01_Introduction.tex}

%%%%%%%%%%% 2. Related Works %%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Works}\label{sec2}
\input{02_RelatedWorks.tex}

%%%%%%%%%%% 3.  the Unified Contrastive Fusion Transformer (UCF-Former) Method%%%%%%%%%%%%%%%%%%%%%%%%
\section{Unified Contrastive Fusion Transformer (UCF-Former)}\label{sec3}
\input{03_Method}

%%%%%%%%%%% 4. Empirical Result %%%%%%%%%%%%%%%%%%%%%%%%
\section{Empirical Results}\label{sec4}
\input{04_Experiments.tex}

%%%%%%%%%%% 5. Conclusion %%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}\label{sec5}
\input{05_Conclusion.tex}

% \section*{Acknowledgments}
% This should be a simple paragraph before the References to thank those individuals and institutions who have supported your work on this article.
%%%%%%%%%%% Reference %%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{IEEEtran}
\bibliography{Reference}    
%%%%%%%%%%% Biography %%%%%%%%%%%%%%%%%%%%%%%%
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Figure/Koyang.jpeg}}]{Kyoung Ok Yang} is currently working toward the PhD degree in artificial intelligent from Hanyang University, Seoul, South Korea. Her research interests include image processing and multimodal fusion Network.\end{IEEEbiography}
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Figure/jhKoh.jpeg}}]{Junho Koh} is currently working toward the PhD degree in electrical engineering from Hanyang University, Seoul, South Korea. His research interests include object detection, multi-object tracking, and 3D perception for self-driving. \end{IEEEbiography}
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Figure/pro_jwchoi.jpeg}}]{Jun Won Choi}
% In this paragraph you can place your educational, professional background and research and other interests.\end{IEEEbiography}

\end{document}